{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk_CPgEIT4bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "try:\n",
        "    import rawpy\n",
        "except ModuleNotFoundError:\n",
        "    !pip3 install rawpy\n",
        "    import rawpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfR9WaVREgZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWYK_Uj0X1rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dir = '/content/drive/My Drive/ImageDataset/Sony/Sony/short/'\n",
        "gt_dir = '/content/drive/My Drive/ImageDataset/Sony/Sony/long/'\n",
        "checkpoint_dir = '/content/drive/My Drive/CheckPoint3/'\n",
        "result_dir = '/content/drive/My Drive/Results3/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8U8jtvPTAFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGbUKx6DSQCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "if not os.path.isdir(result_dir):\n",
        "    os.makedirs(result_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06-x-ChWTlsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gt_dir_2 = '/content/drive/My Drive/ImageDataset/Sony/Sony_gt_16bitPNG'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fOrVQTmeU7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_fns = glob.glob(gt_dir + \"0*.ARW\")\n",
        "train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voj7z-Xaeuks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = 512\n",
        "save_freq = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVMbsnad3Rbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7734528-71f3-4bbd-8701-d88c5c7005b9"
      },
      "source": [
        "d =10\n",
        "a=1\n",
        "if d:\n",
        "    a=2\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5Ds90wDez-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEBUG = 0\n",
        "if DEBUG:\n",
        "    save_freq = 2\n",
        "    train_ids = np.random.choice(train_ids, 161)\n",
        "    # train_ids = train_ids[0:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nC6KXbnX3oz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1becfd8-bb56-4f00-8d0a-0eede17ff0c2"
      },
      "source": [
        "train_ids[80:88]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[110, 112, 113, 114, 117, 118, 119, 121]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWJsx6qVk5rW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW8jHWWu1Tsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def expansive_block(in_channels, mid_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(kernel_size=3, in_channels=in_channels, out_channels=mid_channels, padding=1),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.BatchNorm2d(mid_channels),\n",
        "        nn.Conv2d(kernel_size=3, in_channels=mid_channels, out_channels=mid_channels, padding=1),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.BatchNorm2d(mid_channels),\n",
        "        nn.ConvTranspose2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_RYhZSW28wC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def final_block(in_channels, mid_channels, out_channels, kernel_size=3):\n",
        "    return nn.Sequential(\n",
        "            nn.Conv2d(kernel_size=kernel_size, in_channels=in_channels, out_channels=mid_channels, padding=1),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.Conv2d(kernel_size=kernel_size, in_channels=mid_channels, out_channels=mid_channels, padding=1),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.Conv2d(kernel_size=1, in_channels=mid_channels, out_channels=out_channels, padding=0),\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n-1wzWj-ekL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DepthToSpace(nn.Module):\n",
        "\n",
        "    def __init__(self, block_size):\n",
        "        super().__init__()\n",
        "        self.bs = block_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()\n",
        "        x = x.view(N, self.bs, self.bs, C // (self.bs ** 2), H, W)  # (N, bs, bs, C//bs^2, H, W)\n",
        "        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # (N, C//bs^2, H, bs, W, bs)\n",
        "        x = x.view(N, C // (self.bs ** 2), H * self.bs, W * self.bs)  # (N, C//bs^2, H * bs, W * bs)\n",
        "        # print(\"In DS shape \"+str(x.shape))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IR5m12gH_Xu7",
        "colab": {}
      },
      "source": [
        "def pack_raw(raw):\n",
        "    #packing bayer sensor image to 4 channels\n",
        "    im = raw.raw_image_visible.astype(np.float32)\n",
        "    im = np.maximum(im - 512, 0)/(16383 - 512) #subtracting the black level\n",
        "    # print(im.shape)\n",
        "    im = np.expand_dims(im, axis=0)\n",
        "    img_shape = im.shape\n",
        "    # print(img_shape)\n",
        "    H = img_shape[1]\n",
        "    W = img_shape[2]\n",
        "    # The \"channels\" channel might be needed to be moved ahead(in that case axis = 0)\n",
        "    out = np.concatenate((im[:, 0:H:2, 0:W:2],\n",
        "                          im[:, 0:H:2, 1:W:2],\n",
        "                          im[:,1:H:2, 1:W:2],\n",
        "                          im[:, 1:H:2, 0:W:2],\n",
        "                          ), axis=0)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjwdTY_Kam6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gt_images = [None] * 6000\n",
        "# input_images = {}\n",
        "# input_images['300'] = [None] * len(train_ids)\n",
        "# input_images['250'] = [None] * len(train_ids)\n",
        "# input_images['100'] = [None] * len(train_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bagub75Wf17D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g_loss = np.zeros((5000, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt5iucejf51F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allfolders = glob.glob(result_dir + '*0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl9JXwnkgg6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qwnrzXdmQWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o5_tuOECr8_5",
        "colab": {}
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "        self.dconv_down1 = double_conv(4, 32)\n",
        "        self.dconv_down2 = double_conv(32, 64)\n",
        "        self.dconv_down3 = double_conv(64, 128)\n",
        "        self.dconv_down4 = double_conv(128, 256)\n",
        "        self.dconv_down5 = double_conv(256, 512)\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.conv2d_t = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.dconv_up1 = expansive_block(in_channels = 512, mid_channels = 256, out_channels = 128)\n",
        "        self.dconv_up2 = expansive_block(in_channels = 256, mid_channels = 128, out_channels = 64)\n",
        "        self.dconv_up3 = expansive_block(in_channels = 128, mid_channels = 64, out_channels = 32)\n",
        "        self.final_layer = final_block(in_channels = 64, mid_channels = 32, out_channels = n_out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.dconv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "        # print(\"Con1= \"+str(conv1.shape))\n",
        "        conv2 = self.dconv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "        # print(\"Con2= \"+str(conv2.shape))\n",
        "        conv3 = self.dconv_down3(x)\n",
        "        x = self.maxpool(conv3)\n",
        "        # print(\"Con3 = \"+ str(conv3.shape))\n",
        "        conv4 = self.dconv_down4(x)\n",
        "        x = self.maxpool(conv4)\n",
        "        # print(\"Con4= \"+str(conv4.shape))\n",
        "        conv5 = self.dconv_down5(x)\n",
        "        # print(\"Con5= \"+str(conv5.shape))\n",
        "        # First upsampling\n",
        "        x = self.conv2d_t(conv5)\n",
        "        # print(\"First upcon+conv4 = \"+str(x.shape))\n",
        "        #concatenation and up_conv(Includes 2 conv+up_conv)\n",
        "       \n",
        "        x = torch.cat([x, conv4], dim = 1)\n",
        "        #256+256 channels as ip\n",
        "        x = self.dconv_up1(x)\n",
        "        # print(\"Second upcon+conv3 = \"+str(x.shape))\n",
        "        #has 128 channels op\n",
        "        x = torch.cat([x, conv3], dim = 1)\n",
        "        #128+128 channels as ip\n",
        "        x = self.dconv_up2(x)\n",
        "        # has 64 channels op\n",
        "        # print(\"third upcon+conv2 = \"+str(x.shape))\n",
        "        x = torch.cat([x, conv2], dim = 1)\n",
        "        #64+64 channels as ip\n",
        "        x = self.dconv_up3(x)\n",
        "        # has 32 channels op\n",
        "        # print(\"fourth upcon+conv1 = \"+str(x.shape))\n",
        "        x = torch.cat([x, conv1], dim = 1)\n",
        "        # has 32+32 channels\n",
        "        x = self.final_layer(x)\n",
        "        # print(\"pre final shape = \", str(x.shape))\n",
        "        x = DepthToSpace(2)(x)\n",
        "        # print(\"X shape = \"+str(x.shape))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIiU9L-WB3L3",
        "colab_type": "text"
      },
      "source": [
        "Load the latest saved model for further processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA42jMAvE1f6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dirs = [checkpoint_dir+str(d) for d in os.listdir(checkpoint_dir)]\n",
        "dirs  = sorted(dirs, reverse=True)\n",
        "# dirs = sorted(dirs, key=lambda x: os.path.getctime(x), reverse=True)[:1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiY-X-F-bOET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65142cd1-2c5d-4b30-931b-c84d03c4818a"
      },
      "source": [
        "dirs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/CheckPoint3/0010-model.ckpt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuviYBspTG35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if dirs == []:\n",
        "    train_epoch = None\n",
        "else:\n",
        "    train_epoch = int(dirs[0][-15:-11])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nCGlnTqSEXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7164c5c-1223-4f03-b5b5-1770f2b94c27"
      },
      "source": [
        "train_epoch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlgOTK9lKnXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdn1o0i_1cUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = UNet(12)\n",
        "net = net.to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr = 1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "if train_epoch != None:\n",
        "    checkpoint = torch.load(checkpoint_dir+'/%04d-model.ckpt'%train_epoch)\n",
        "    net.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    train_epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXZFEEIRv1Kh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "162cc186-4122-463f-c000-a94b7f100004"
      },
      "source": [
        "loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1735, device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA-_PGCv_Ccf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for state in optimizer.state.values():\n",
        "        for k, v in state.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                state[k] = v.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZRKPYBQrmcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if train_epoch == None:\n",
        "    train_epoch = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al0qTdx00Eak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def abs_L1_loss(output, target):\n",
        "    G_loss = torch.mean(torch.abs(output - target))\n",
        "    return G_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRN2yllfTVKV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c24505f1-7b92-4e1c-ae16-674890979615"
      },
      "source": [
        "len(train_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYrosZwqaAV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzsmxtqcgjrX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60135f0d-614e-458a-ed22-83853cca5c7c"
      },
      "source": [
        "for epoch in range(train_epoch+1, 50+train_epoch+1):\n",
        "    print(epoch)\n",
        "    ind_list = []\n",
        "    # print(epoch)\n",
        "    if(os.path.isfile(checkpoint_dir+\"%04d\"%epoch)):\n",
        "        lo\n",
        "        continue\n",
        "    cnt = 0\n",
        "    for ind in np.random.permutation(len(train_ids)):\n",
        "        # print(\"INd \" + str(ind))\n",
        "        train_id = train_ids[ind]\n",
        "        in_files = glob.glob(input_dir + \"%05d_00*.ARW\" %train_id)\n",
        "        # print(\"len \"+str(len(in_files)))\n",
        "        if(len(in_files) == 1):\n",
        "            idx = 0\n",
        "        else:\n",
        "            idx = np.random.randint(0, len(in_files)-1)\n",
        "        in_path = in_files[idx]\n",
        "        in_fn = os.path.basename(in_path)\n",
        "\n",
        "        gt_files = glob.glob(gt_dir + \"%05d_00*.ARW\" %train_id)\n",
        "        gt_path = gt_files[0]\n",
        "        gt_fn = os.path.basename(gt_path)\n",
        "\n",
        "        in_exposure = float(in_fn[9:-5])\n",
        "        gt_exposure = float(gt_fn[9:-5])\n",
        "        ratio = min(gt_exposure / in_exposure, 300) #300 can be varied\n",
        "\n",
        "        st = time.time()\n",
        "        cnt += 1\n",
        "\n",
        "        # if gt_images[ind] is None:\n",
        "        if ind not in ind_list:\n",
        "            ind_list.append(ind)\n",
        "            raw = rawpy.imread(in_path)\n",
        "            ip_raw = np.expand_dims(pack_raw(raw), axis = 0) * ratio\n",
        "            ## memory consumption is extreme\n",
        "            # input_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis = 0) * ratio\n",
        "            ## FOr full size\n",
        "            \n",
        "            gt_raw = rawpy.imread(gt_path)\n",
        "            im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
        "            gt_image = np.expand_dims(np.float32(im / 65535.0), axis=0)\n",
        "            \n",
        "            ## For preprocessed images\n",
        "            \n",
        "            # im = cv2.imread(gt_path)\n",
        "            # gt_image = np.expand_dims(np.float32(im / 65535.0), axis=0)\n",
        "            \n",
        "            ## print(\"Shape \"+ str(gt_images[ind].shape))\n",
        "        \n",
        "        # H = input_images[str(ratio)[0:3]][ind].shape[2]\n",
        "        # W = input_images[str(ratio)[0:3]][ind].shape[3]\n",
        "        H = ip_raw.shape[2]\n",
        "        W = ip_raw.shape[3]\n",
        "        ## print(\"H \"+str(H))\n",
        "        ## print(\"W \"+str(W))\n",
        "        ## TO extract random patch of the image\n",
        "        xx = np.random.randint(0, W - ps)\n",
        "        yy = np.random.randint(0, H - ps)\n",
        "        xx_ = xx*2\n",
        "        yy_ = yy*2\n",
        "        ps_ = ps*2\n",
        "        # print(\"xx_ \"+str(xx_))\n",
        "        # print(\"yy_ \"+str(yy_))\n",
        "        # print(\"ps_ \"+str(ps_))\n",
        "        # input_patch = input_images[str(ratio)[0:3]][ind][:, :,yy:yy+ps, xx:xx+ps]\n",
        "        input_patch = ip_raw[:, :,yy:yy+ps, xx:xx+ps]\n",
        "        gt_patch = gt_image[:, yy_:yy_ + ps_, xx_:xx_ + ps_, :]  #not sure\n",
        "        # gt_patch = gt_images[ind][:, yy:yy+ps, xx:xx+ps, :]\n",
        "        # print(\"gt image shape \"+str(gt_images[ind].shape))\n",
        "        # print(\"gt_patch drawn np=\"+str(gt_patch.shape))\n",
        "        gt_patch = np.transpose(gt_patch, (0, 3, 1, 2))\n",
        "        # print(\"gt_patch np=\"+str(gt_patch.shape))\n",
        "        #manual data augmentation for the patch\n",
        "        if np.random.randint(2, size=1)[0] == 1: #random flip\n",
        "            input_patch = np.flip(input_patch, axis = 2)\n",
        "            gt_patch = np.flip(gt_patch, axis = 2)\n",
        "        if np.random.randint(2, size=1)[0] == 1: #vertical flip\n",
        "            input_patch = np.flip(input_patch, axis = 3)\n",
        "            gt_patch = np.flip(gt_patch, axis = 3)\n",
        "        if np.random.randint(2, size=1)[0] == 1: #random transpse\n",
        "            input_patch = np.transpose(input_patch, (0, 1, 3, 2))\n",
        "            gt_patch = np.transpose(gt_patch, (0, 1, 3, 2))\n",
        "        \n",
        "        input_patch = np.minimum(input_patch, 1.0)\n",
        "        # if torch.cuda.\n",
        "        input_patch =torch.tensor(input_patch, device=device).float()\n",
        "        gt_patch = np.ascontiguousarray(gt_patch)\n",
        "        gt_patch = torch.from_numpy(gt_patch).float().to(device)\n",
        "        # print(\"gtpatch =\" +str(gt_patch.shape))\n",
        "        # print(\"Ip patch shape = \"+str(input_patch.shape))\n",
        "        optimizer.zero_grad()\n",
        "        out = net(input_patch)\n",
        "        scheduler.step()\n",
        "        # print(type(out))\n",
        "        # print(\"Op shape = \" + str(out.shape))\n",
        "        loss = abs_L1_loss(out, gt_patch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        out = out.detach().cpu().numpy()\n",
        "        out = np.minimum(np.maximum(out, 0), 1)\n",
        "        # print(\"Out shape =\"+str(out.shape))\n",
        "        g_loss[ind] = loss.detach().cpu()\n",
        "\n",
        "        print(\"%d %d Loss=%.5f Time=%.3f\" % (epoch, cnt, np.mean(g_loss[np.where(g_loss)]), time.time() - st))\n",
        "        if epoch % save_freq == 0:\n",
        "            if not os.path.isdir(result_dir + '/%04d' % epoch):\n",
        "                os.makedirs(result_dir + '/%04d' % epoch)\n",
        "            gt_patch = gt_patch.cpu().numpy()\n",
        "            # out = out.detach().cpu().numpy()\n",
        "            # print(\"GTS = \"+str(gt_patch.shape))\n",
        "            # print(\"OPS = \"+str(out.shape))\n",
        "            temp = np.concatenate((gt_patch[0, :, :, :], out[0, :, :, :]), axis=2)\n",
        "            # print(\"TP = \"+str(temp.shape))\n",
        "            temp = np.transpose(temp, (1, 2, 0))\n",
        "            temp = np.reshape(temp, (temp.shape[0], temp.shape[1], 3))\n",
        "            Image.fromarray((temp * 255).astype(np.uint8)).save(\n",
        "                result_dir + '/%04d/%05d_00_train_%d.jpg' % (epoch, train_id, ratio))\n",
        "        \n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': net.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "        }, checkpoint_dir+'/%04d-model.ckpt' %epoch)\n",
        "    # if epoch % 10 == 0:\n",
        "    #     train_ids = np.random.choice(train_ids, 80)   \n",
        "        # if cnt % 50 == 0:\n",
        "        #     torch.cuda.clear_cache()\n",
        "        # running_loss += loss.item()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11 1 Loss=0.27304 Time=1.787\n",
            "11 2 Loss=0.16755 Time=2.789\n",
            "11 3 Loss=0.14418 Time=1.681\n",
            "11 4 Loss=0.13681 Time=3.016\n",
            "11 5 Loss=0.12381 Time=3.122\n",
            "11 6 Loss=0.13835 Time=1.639\n",
            "11 7 Loss=0.14443 Time=3.064\n",
            "11 8 Loss=0.13840 Time=2.914\n",
            "11 9 Loss=0.14754 Time=2.725\n",
            "11 10 Loss=0.14364 Time=3.047\n",
            "11 11 Loss=0.13723 Time=1.621\n",
            "11 12 Loss=0.12994 Time=1.677\n",
            "11 13 Loss=0.12302 Time=2.822\n",
            "11 14 Loss=0.12197 Time=2.814\n",
            "11 15 Loss=0.11913 Time=3.196\n",
            "11 16 Loss=0.11483 Time=2.673\n",
            "11 17 Loss=0.11277 Time=2.729\n",
            "11 18 Loss=0.11324 Time=1.774\n",
            "11 19 Loss=0.11192 Time=2.734\n",
            "11 20 Loss=0.12160 Time=1.701\n",
            "11 21 Loss=0.11906 Time=2.268\n",
            "11 22 Loss=0.11639 Time=3.134\n",
            "11 23 Loss=0.11571 Time=2.231\n",
            "11 24 Loss=0.11895 Time=2.796\n",
            "11 25 Loss=0.12114 Time=1.681\n",
            "11 26 Loss=0.12068 Time=2.845\n",
            "11 27 Loss=0.12002 Time=1.713\n",
            "11 28 Loss=0.11906 Time=2.868\n",
            "11 29 Loss=0.11930 Time=3.065\n",
            "11 30 Loss=0.12100 Time=2.915\n",
            "11 31 Loss=0.12460 Time=2.088\n",
            "11 32 Loss=0.12305 Time=2.241\n",
            "11 33 Loss=0.12319 Time=2.906\n",
            "11 34 Loss=0.12222 Time=1.714\n",
            "11 35 Loss=0.12364 Time=1.747\n",
            "11 36 Loss=0.12207 Time=3.172\n",
            "11 37 Loss=0.12243 Time=1.722\n",
            "11 38 Loss=0.12261 Time=1.612\n",
            "11 39 Loss=0.12154 Time=1.690\n",
            "11 40 Loss=0.12042 Time=3.166\n",
            "11 41 Loss=0.12028 Time=1.611\n",
            "11 42 Loss=0.11911 Time=2.655\n",
            "11 43 Loss=0.12024 Time=2.336\n",
            "11 44 Loss=0.11996 Time=2.744\n",
            "11 45 Loss=0.11851 Time=1.786\n",
            "11 46 Loss=0.11711 Time=2.669\n",
            "11 47 Loss=0.11570 Time=3.069\n",
            "11 48 Loss=0.11468 Time=2.616\n",
            "11 49 Loss=0.11330 Time=1.644\n",
            "11 50 Loss=0.11313 Time=1.691\n",
            "11 51 Loss=0.11403 Time=2.778\n",
            "11 52 Loss=0.11297 Time=2.843\n",
            "11 53 Loss=0.11186 Time=2.229\n",
            "11 54 Loss=0.11200 Time=1.651\n",
            "11 55 Loss=0.11149 Time=3.207\n",
            "11 56 Loss=0.11183 Time=3.294\n",
            "11 57 Loss=0.11241 Time=3.077\n",
            "11 58 Loss=0.11794 Time=2.908\n",
            "11 59 Loss=0.11747 Time=2.830\n",
            "11 60 Loss=0.11661 Time=2.867\n",
            "11 61 Loss=0.11796 Time=2.888\n",
            "11 62 Loss=0.11691 Time=1.672\n",
            "11 63 Loss=0.11597 Time=2.613\n",
            "11 64 Loss=0.11493 Time=3.253\n",
            "11 65 Loss=0.11381 Time=2.756\n",
            "11 66 Loss=0.11286 Time=1.606\n",
            "11 67 Loss=0.11211 Time=2.662\n",
            "11 68 Loss=0.11177 Time=2.790\n",
            "11 69 Loss=0.11085 Time=3.027\n",
            "11 70 Loss=0.11016 Time=1.587\n",
            "11 71 Loss=0.11085 Time=3.230\n",
            "11 72 Loss=0.11040 Time=3.231\n",
            "11 73 Loss=0.11069 Time=2.628\n",
            "11 74 Loss=0.11020 Time=3.346\n",
            "11 75 Loss=0.11023 Time=3.329\n",
            "11 76 Loss=0.11141 Time=1.690\n",
            "11 77 Loss=0.11218 Time=3.023\n",
            "11 78 Loss=0.11144 Time=1.603\n",
            "11 79 Loss=0.11090 Time=2.923\n",
            "11 80 Loss=0.11010 Time=1.654\n",
            "11 81 Loss=0.10999 Time=1.691\n",
            "11 82 Loss=0.10966 Time=2.111\n",
            "11 83 Loss=0.11021 Time=2.143\n",
            "11 84 Loss=0.10973 Time=3.118\n",
            "11 85 Loss=0.11228 Time=2.766\n",
            "11 86 Loss=0.11225 Time=3.108\n",
            "11 87 Loss=0.11255 Time=2.603\n",
            "11 88 Loss=0.11195 Time=2.952\n",
            "11 89 Loss=0.11323 Time=3.109\n",
            "11 90 Loss=0.11256 Time=1.625\n",
            "11 91 Loss=0.11342 Time=2.781\n",
            "11 92 Loss=0.11374 Time=1.609\n",
            "11 93 Loss=0.11350 Time=3.201\n",
            "11 94 Loss=0.11294 Time=1.585\n",
            "11 95 Loss=0.11270 Time=3.039\n",
            "11 96 Loss=0.11210 Time=2.782\n",
            "11 97 Loss=0.11181 Time=1.639\n",
            "11 98 Loss=0.11121 Time=2.814\n",
            "11 99 Loss=0.11129 Time=2.687\n",
            "11 100 Loss=0.11086 Time=2.959\n",
            "11 101 Loss=0.11062 Time=2.863\n",
            "11 102 Loss=0.11051 Time=1.675\n",
            "11 103 Loss=0.11082 Time=1.594\n",
            "11 104 Loss=0.11042 Time=2.823\n",
            "11 105 Loss=0.11012 Time=1.645\n",
            "11 106 Loss=0.10985 Time=2.946\n",
            "11 107 Loss=0.10933 Time=2.686\n",
            "11 108 Loss=0.10960 Time=2.665\n",
            "11 109 Loss=0.10903 Time=1.624\n",
            "11 110 Loss=0.10922 Time=2.796\n",
            "11 111 Loss=0.11000 Time=2.374\n",
            "11 112 Loss=0.11012 Time=1.642\n",
            "11 113 Loss=0.10960 Time=2.603\n",
            "11 114 Loss=0.10933 Time=3.194\n",
            "11 115 Loss=0.11088 Time=1.655\n",
            "11 116 Loss=0.11058 Time=1.664\n",
            "11 117 Loss=0.11038 Time=3.085\n",
            "11 118 Loss=0.11025 Time=1.643\n",
            "11 119 Loss=0.11012 Time=3.341\n",
            "11 120 Loss=0.11061 Time=3.253\n",
            "11 121 Loss=0.11062 Time=2.666\n",
            "11 122 Loss=0.11025 Time=1.625\n",
            "11 123 Loss=0.11035 Time=1.662\n",
            "11 124 Loss=0.11017 Time=2.176\n",
            "11 125 Loss=0.11018 Time=2.990\n",
            "11 126 Loss=0.10961 Time=2.701\n",
            "11 127 Loss=0.10938 Time=1.738\n",
            "11 128 Loss=0.11099 Time=2.721\n",
            "11 129 Loss=0.11051 Time=2.967\n",
            "11 130 Loss=0.11007 Time=1.744\n",
            "11 131 Loss=0.10967 Time=1.616\n",
            "11 132 Loss=0.10925 Time=2.884\n",
            "11 133 Loss=0.10940 Time=2.738\n",
            "11 134 Loss=0.10914 Time=2.478\n",
            "11 135 Loss=0.10888 Time=3.072\n",
            "11 136 Loss=0.10840 Time=3.055\n",
            "11 137 Loss=0.10891 Time=2.702\n",
            "11 138 Loss=0.10849 Time=3.582\n",
            "11 139 Loss=0.10808 Time=2.906\n",
            "11 140 Loss=0.10763 Time=5.789\n",
            "11 141 Loss=0.10756 Time=1.801\n",
            "11 142 Loss=0.10808 Time=2.957\n",
            "11 143 Loss=0.10932 Time=1.802\n",
            "11 144 Loss=0.10916 Time=3.348\n",
            "11 145 Loss=0.10940 Time=1.902\n",
            "11 146 Loss=0.10961 Time=1.864\n",
            "11 147 Loss=0.10924 Time=1.958\n",
            "11 148 Loss=0.10985 Time=1.710\n",
            "11 149 Loss=0.11144 Time=1.803\n",
            "11 150 Loss=0.11112 Time=3.149\n",
            "11 151 Loss=0.11069 Time=2.785\n",
            "11 152 Loss=0.11076 Time=3.480\n",
            "11 153 Loss=0.11061 Time=1.852\n",
            "11 154 Loss=0.11022 Time=2.021\n",
            "11 155 Loss=0.10980 Time=3.011\n",
            "11 156 Loss=0.10962 Time=3.328\n",
            "11 157 Loss=0.10932 Time=3.045\n",
            "11 158 Loss=0.10908 Time=1.876\n",
            "11 159 Loss=0.10896 Time=1.871\n",
            "11 160 Loss=0.10897 Time=2.521\n",
            "11 161 Loss=0.10913 Time=2.905\n",
            "12\n",
            "12 1 Loss=0.10908 Time=1.645\n",
            "12 2 Loss=0.10963 Time=1.668\n",
            "12 3 Loss=0.11011 Time=2.349\n",
            "12 4 Loss=0.11066 Time=1.621\n",
            "12 5 Loss=0.11083 Time=1.636\n",
            "12 6 Loss=0.11071 Time=2.016\n",
            "12 7 Loss=0.11045 Time=1.706\n",
            "12 8 Loss=0.11042 Time=1.629\n",
            "12 9 Loss=0.11040 Time=1.647\n",
            "12 10 Loss=0.11026 Time=1.681\n",
            "12 11 Loss=0.11026 Time=1.625\n",
            "12 12 Loss=0.11023 Time=1.636\n",
            "12 13 Loss=0.11024 Time=1.649\n",
            "12 14 Loss=0.11027 Time=1.669\n",
            "12 15 Loss=0.11004 Time=1.687\n",
            "12 16 Loss=0.10903 Time=1.604\n",
            "12 17 Loss=0.10937 Time=1.667\n",
            "12 18 Loss=0.10979 Time=1.658\n",
            "12 19 Loss=0.11008 Time=1.651\n",
            "12 20 Loss=0.11008 Time=1.729\n",
            "12 21 Loss=0.10979 Time=1.663\n",
            "12 22 Loss=0.10976 Time=1.724\n",
            "12 23 Loss=0.10976 Time=1.655\n",
            "12 24 Loss=0.10975 Time=1.830\n",
            "12 25 Loss=0.11005 Time=1.683\n",
            "12 26 Loss=0.11037 Time=1.665\n",
            "12 27 Loss=0.10972 Time=1.690\n",
            "12 28 Loss=0.10934 Time=1.811\n",
            "12 29 Loss=0.10888 Time=1.780\n",
            "12 30 Loss=0.10866 Time=1.701\n",
            "12 31 Loss=0.10869 Time=1.688\n",
            "12 32 Loss=0.10886 Time=2.267\n",
            "12 33 Loss=0.10879 Time=1.688\n",
            "12 34 Loss=0.10930 Time=1.860\n",
            "12 35 Loss=0.10945 Time=1.648\n",
            "12 36 Loss=0.10950 Time=1.695\n",
            "12 37 Loss=0.10930 Time=1.705\n",
            "12 38 Loss=0.10927 Time=1.624\n",
            "12 39 Loss=0.10856 Time=1.709\n",
            "12 40 Loss=0.10855 Time=1.786\n",
            "12 41 Loss=0.10914 Time=1.642\n",
            "12 42 Loss=0.10945 Time=1.692\n",
            "12 43 Loss=0.10970 Time=2.358\n",
            "12 44 Loss=0.10987 Time=1.632\n",
            "12 45 Loss=0.10939 Time=1.664\n",
            "12 46 Loss=0.10946 Time=1.621\n",
            "12 47 Loss=0.10963 Time=1.781\n",
            "12 48 Loss=0.10969 Time=1.706\n",
            "12 49 Loss=0.10956 Time=1.706\n",
            "12 50 Loss=0.10997 Time=1.662\n",
            "12 51 Loss=0.10987 Time=1.794\n",
            "12 52 Loss=0.10936 Time=1.681\n",
            "12 53 Loss=0.10920 Time=1.699\n",
            "12 54 Loss=0.10922 Time=1.717\n",
            "12 55 Loss=0.10908 Time=1.666\n",
            "12 56 Loss=0.10951 Time=1.662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr4MD96wAL46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}